
# This save path need to be changed to your own path
# BEGIN
text_data_dir: "[YOUR_DATA_SAVE_DIR]/transformed/LaMAGIC"
target_data: "matrix_form_6comp.json"
LUT_cir_data_name: "matrix_form_6comp.json"
tokenized_data_dir: "[YOUR_DATA_SAVE_DIR]/transformed/LaMAGIC/tokenized"
tokenized_data_trn: "matrix_form_6comp_trn.pickle"
tokenized_data_val: "matrix_form_6comp_val.pickle"
base_model: "[YOUR_MODEL_SAVE_DIR]/flan-t5-base"
# END

order: 'duty vertex edge'

task: 'maskedGen'
mask_style: 'T5'
llm: 'flan-t5'
load_pretrained: True
finetune_method: 'pure'
masked_method: 'random'
masked_ratio: 0.5
warmup_steps: 300
val_custom: False
data_augment: False

output_dir: "/"
val_set_size: 10000

prompt_template_name: "alpaca"
generate: False

# num_epochs: 10
num_epochs: 5
reg: 0.0
lr: 3.0e-4
batch_size: 128
micro_batch_size: 8
cutoff_len: 512
train_on_inputs: True  # if False, masks out inputs in loss
add_eos_token: False
group_by_length: True
fp16: False
prune_invalid: True
normalize: False
encoder_model_dir: Null


lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
lora_r: 16
lora_alpha: 16
lora_dropout: 0.05
eval_steps: 80
dropout_rate: 0.1
num_labels: 1

wandb_run_name: flanT5-maskedgen-data345
resume_from_checkpoint: Null
use_wandb: True
wandb_project: Analog_LLM
